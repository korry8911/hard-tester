{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import docker\n",
    "import re\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "import random\n",
    "import time\n",
    "import errno\n",
    "import os\n",
    "import signal\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Code\n",
    "\n",
    "def start_blockade(num_nodes, repo_path):\n",
    "    #\"/Users/basho/Desktop/basho/pypsen/blockade.yml\"\n",
    "    text_file = open(repo_path+'/blockade.yml', \"w\")\n",
    "    text_file.close()\n",
    "    text_file = open(repo_path+'/blockade.yml', \"w\")\n",
    "    text_file.write('containers:')\n",
    "    for i in range(1,num_nodes+1):\n",
    "        temp_node_name = 'node'+str(i)\n",
    "        text_file.write(str('\\n    '+temp_node_name+': \\n        image: basho/riak-kv:latest \\n        hostname: '+temp_node_name+' \\n        name: '+temp_node_name+' \\n\\n'))\n",
    "    text_file.close()\n",
    "    os.system(\"(cd \"+repo_path+\" && blockade up)\")\n",
    "    time.sleep(10)\n",
    "    \n",
    "def stop_blockade(repo_path):\n",
    "    os.system(\"(cd \"+repo_path+\" && blockade destroy)\")\n",
    "    os.system(\"(cd \"+repo_path+\" && docker rm -f `docker ps --no-trunc -aq`)\")\n",
    "    \n",
    "def setup_nodes(docker_client):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        items = docker_client.containers()\n",
    "        for i in range(0,len(items)):\n",
    "            node = items[i]['Names'][0][1:]\n",
    "            job = docker_client.exec_create(node, \"sudo apt-get update\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "            job = docker_client.exec_create(node, \"sudo apt-get -f install -y\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "            job = docker_client.exec_create(node, \"sudo apt-get install -y python-pip build-essential libssl-dev libffi-dev python-dev\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "            job = docker_client.exec_create(node, \"sudo pip install six --upgrade\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "            job = docker_client.exec_create(node, \"sudo pip install pyOpenSSL --upgrade\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "            job = docker_client.exec_create(node, \"sudo pip install ipyparallel riak cloudpickle cryptography\")\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "\n",
    "def stop_cluster():\n",
    "    proc = subprocess.Popen([\"ipcluster stop\"], shell=True, stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "    \n",
    "def start_cluster_controller():\n",
    "    proc = subprocess.Popen([\"sudo ipcontroller --ip=$(ipconfig getifaddr en0)\"], shell=True, stdin=None, stdout=None, stderr=None, close_fds=True)\n",
    "    \n",
    "def start_cluster_engines(docker_client):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        os.system(\"sudo chmod -R a+rwX ~/.ipython/profile_default/security/\")\n",
    "        for item in docker_client.containers():\n",
    "            node = item['Names'][0][1:]\n",
    "            print 'Starting engine on '+node\n",
    "            print os.system(\"docker cp ~/.ipython/profile_default/security/ipcontroller-engine.json \"+node+\":/ipcontroller-engine.json\")\n",
    "            job = docker_client.exec_create(node, 'sudo ipengine --file=/ipcontroller-engine.json')\n",
    "            docker_client.exec_start(job['Id'], detach=True)\n",
    "\n",
    "def stop_cluster_engines(docker_client):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for item in docker_client.containers():\n",
    "            node = item['Names'][0][1:]\n",
    "            job = docker_client.exec_create(node, 'ipengine stop')\n",
    "            print docker_client.exec_start(job['Id'])\n",
    "\n",
    "def riak_cluster(docker_client):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        client = docker_client\n",
    "        nodes = []\n",
    "        for item in client.containers():\n",
    "            nodes.append({'name': item['Names'][0][1:],\n",
    "                          'ip': item['NetworkSettings']['Networks']['bridge']['IPAddress']})\n",
    "        print nodes\n",
    "        jobs = []\n",
    "\n",
    "        for i in range(0,len(nodes)):\n",
    "\n",
    "            if i < len(nodes)-1:\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"riak-admin cluster join riak@\"+nodes[len(nodes)-1]['ip']))\n",
    "            else:\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"sleep 3\"))\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"riak-admin cluster plan\"))\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"sleep 3\"))\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"riak-admin cluster commit\"))\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"sleep 3\"))\n",
    "                jobs.append(client.exec_create(nodes[i]['name'],\"riak-admin member-status\"))\n",
    "\n",
    "        for job in jobs:\n",
    "            print client.exec_start(job['Id'])\n",
    "    return nodes\n",
    "\n",
    "def get_riak_nodes(docker_client):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        nodes = []\n",
    "        for item in docker_client.containers():\n",
    "            nodes.append({'name': item['Names'][0][1:],\n",
    "                          'ip': item['NetworkSettings']['Networks']['bridge']['IPAddress']})\n",
    "        print nodes\n",
    "    return nodes\n",
    "\n",
    "#examples\n",
    "#exec_op(nodes[0], nodes[1], {'type': 'write', 'value': 6}, 'test_bucket', 'test_key', 'value', 3000)\n",
    "#exec_op(nodes[2], nodes[0], {'type': 'read'})\n",
    "#exec_op(nodes[0], nodes[1], {'type': 'cas', 'swap_on_val': 6, 'swap_to_val': 5}, 'test_bucket', 'test_key', 'value', 3000)\n",
    "#exec_op(nodes[0], nodes[1], {'type': 'cas', 'swap_on_val': 6, 'swap_to_val': 5}, 'test_bucket', 'test_key', 'value', 3000)\n",
    "#exec_op(nodes[2], nodes[0], {'type': 'sleep', 'sleep_val':3}, 'test_bucket', 'test_key', 'value', 3000)\n",
    "\n",
    "def exec_op(host, target, op, bucket, key, value_field, timeout):\n",
    "    client = riak.RiakClient(host=target['ip'], http_port=8098)\n",
    "    bucket = client.bucket(bucket)\n",
    "    result = {}\n",
    "    result['host'] = host['name']\n",
    "    result['target'] = target['name']\n",
    "    \n",
    "    if op['type'] == 'read':\n",
    "        try:\n",
    "            result['type'] = 'read'\n",
    "            result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "            out = bucket.get(key, timeout=timeout)\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            if out.data[value_field] != None:\n",
    "                result['output'] = str(out.data[value_field])\n",
    "                result['success'] = 'ok'\n",
    "            else:\n",
    "                result['output'] = 'read failed'\n",
    "                result['success'] = 'fail'\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['output'] = 'read timed out'\n",
    "            result['success'] = 'timeout'\n",
    "        \n",
    "    elif op['type'] == 'write':\n",
    "        result['type'] = 'write'\n",
    "        result['input'] = str(op['value'])\n",
    "        new_val = {value_field: op['value']}\n",
    "        new_obj = bucket.new(key, data=new_val)\n",
    "        try:\n",
    "            result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "            new_obj.store(timeout=timeout)\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['output'] = str(op['value'])\n",
    "            result['success'] = 'ok'\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['output'] = 'write timed out'\n",
    "            result['success'] = 'timeout'\n",
    "        \n",
    "    elif op['type'] == 'cas':\n",
    "        result['type'] = 'cas'\n",
    "        result['input'] = '[' + str(op['swap_on_val']) + ' ' + str(op['swap_to_val']) + ']'\n",
    "        try:\n",
    "            result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "            out = bucket.get(key, timeout=timeout)\n",
    "            if out.data[value_field] != None:\n",
    "                if out.data[value_field] == op['swap_on_val']:\n",
    "                    new_val = {value_field: op['swap_to_val']}\n",
    "                    new_obj = bucket.new(key, data=new_val)\n",
    "                    try:\n",
    "                        new_obj.store(timeout=timeout)\n",
    "                        result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "                        result['output'] = '[' + str(op['swap_on_val']) + ' ' + str(op['swap_to_val']) + ']'\n",
    "                        result['success'] = 'ok'\n",
    "                    except Exception as e:\n",
    "                        print e\n",
    "                        result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "                        result['output'] = 'cas timed out'\n",
    "                        result['success'] = 'timeout'\n",
    "                else:\n",
    "                    result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "                    result['output'] = 'no swap'\n",
    "                    result['success'] = 'fail'\n",
    "            else:\n",
    "                result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "                result['output'] = 'swap read failed '\n",
    "                result['success'] = 'fail'\n",
    "        except Exception as e:\n",
    "            print e\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['output'] = 'cas timed out'\n",
    "            result['success'] = 'timeout'\n",
    "            \n",
    "    elif op['type'] == 'sleep':\n",
    "        result['type'] = 'sleep'\n",
    "        result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "        time.sleep(op['sleep_val'])\n",
    "        result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "        result['output'] = 'sleep val: '+ str(op['sleep_val'])\n",
    "        result['success'] = 'ok'\n",
    "    else:\n",
    "        result['output'] = 'bad op type'\n",
    "        \n",
    "    return result\n",
    "\n",
    "def start_parallel_executor():\n",
    "    rc = ipp.Client()\n",
    "    dview = rc[:]\n",
    "    dview.use_cloudpickle()\n",
    "    return dview\n",
    "\n",
    "def run_test(run_cmds, bucket_name, key_name, value_field, timeout):\n",
    "    results= []\n",
    "    for cmd in run_cmds:\n",
    "        time.sleep(random.random()*1)\n",
    "        res = exec_op(cmd['host'], cmd['target'], cmd['op'], bucket_name, key_name, value_field, timeout)\n",
    "        results.append(res)\n",
    "    return results\n",
    "\n",
    "def gen_cmds(nodes, node, num_cmds):\n",
    "    #start with a write to avoid read failures\n",
    "    host = node\n",
    "    target_list = nodes\n",
    "    cmds = [{'host': host, 'target': host, 'op': {'type': 'write', 'value': int(random.random()*10)}}]\n",
    "    for x in range(1,num_cmds):\n",
    "        \n",
    "        new_op = {}\n",
    "        new_cmd = {}\n",
    "        new_target = random.choice(target_list)\n",
    "        selector = random.randint(1,4)\n",
    "        \n",
    "        #Read\n",
    "        if selector == 1:\n",
    "            new_op = {'type': 'read'}\n",
    "            new_cmd = {'host': host, 'target': new_target, 'op': new_op}\n",
    "\n",
    "        #Write\n",
    "        elif selector == 2:\n",
    "            new_op = {'type': 'write', 'value': int(random.random()*10)}\n",
    "            new_cmd = {'host': host, 'target': new_target, 'op': new_op}\n",
    "            \n",
    "        #CAS\n",
    "        elif selector == 3:\n",
    "            new_op = {'type': 'cas', 'swap_on_val': int(random.random()*10), 'swap_to_val': int(random.random()*10)}\n",
    "            new_cmd = {'host': host, 'target': new_target, 'op': new_op}\n",
    "\n",
    "        #Sleep\n",
    "        elif selector == 4:\n",
    "            new_op = {'type': 'sleep', 'sleep_val':random.random()*3}\n",
    "            new_cmd = {'host': host, 'target': new_target, 'op': new_op}\n",
    "\n",
    "        else:\n",
    "            print 'bad selector'\n",
    "        cmds.append(new_cmd)\n",
    "    return cmds\n",
    "\n",
    "def start_consitency_test(parallel_executor, nodes, num_commands, bucket_name, key_name, value_field, timeout, repo_path):\n",
    "    parallel_results = parallel_executor.map(lambda node: run_test(gen_cmds(nodes, node, num_commands), bucket_name, key_name, value_field, timeout), nodes)\n",
    "    nemisis_up = False\n",
    "    node_names = [\"node\"+str(i) for i in range(1,len(nodes)+1)]\n",
    "    nemisis_results = []\n",
    "    while not parallel_results.done():\n",
    "        result = {}\n",
    "        result['host'] = 'nemisis'\n",
    "        if nemisis_up:\n",
    "            time.sleep(5)\n",
    "            result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "            os.system(\"(cd \"+repo_path+\" && blockade join)\")\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['type'] = 'join'\n",
    "            result['success'] = 'ok'\n",
    "            result['output'] = 'blockade join'\n",
    "            nemisis_up = False\n",
    "\n",
    "        else:\n",
    "            rand_nodes = random.sample(set(node_names), random.randint(1,len(node_names)))\n",
    "            cmd = ''\n",
    "            for node in rand_nodes:\n",
    "                cmd = cmd + node + ','\n",
    "            cmd = cmd[:-1]\n",
    "            result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "            os.system(\"(cd \"+repo_path+\" && blockade partition \"+cmd+\")\")\n",
    "            result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "            result['type'] = 'partition'\n",
    "            result['success'] = 'ok'\n",
    "\n",
    "            out = '\"Cut off {'\n",
    "            partA = list(set(node_names)-set(rand_nodes))\n",
    "            for node in node_names:\n",
    "                cmd = ':'+node+' #{:'\n",
    "                if node in rand_nodes:\n",
    "                    for other_node in partA:\n",
    "                        cmd = cmd+other_node+' '\n",
    "                if node in partA:\n",
    "                    for other_node in rand_nodes:\n",
    "                        cmd = cmd+other_node+' '\n",
    "                cmd = cmd.strip()+'}, '\n",
    "                out = out + cmd\n",
    "            out = out.strip()[:-1]+'}\"'\n",
    "            result['output'] = out\n",
    "            nemisis_up = True\n",
    "\n",
    "        nemisis_results.append(result)\n",
    "    if nemisis_up:\n",
    "        result['invoke_timestamp'] = int(round(time.time() * 1000))\n",
    "        os.system(\"(cd \"+repo_path+\" && blockade join)\")\n",
    "        result['compl_timestamp'] = int(round(time.time() * 1000))\n",
    "        result['type'] = 'join'\n",
    "        result['success'] = 'ok'\n",
    "        result['output'] = 'blockade join'\n",
    "        nemisis_up = False\n",
    "        nemisis_results.append(result)\n",
    "\n",
    "    pr = parallel_results.result()\n",
    "    pr.append(nemisis_results)\n",
    "    return pr\n",
    "\n",
    "def make_causal_history(raw_history):\n",
    "    history = [item for sublist in raw_history for item in sublist]\n",
    "    causal_history = sorted(history, key=lambda k: k['invoke_timestamp'])\n",
    "    print 'History length: '+ str(len(causal_history))\n",
    "    num_timeouts = [item for item in causal_history if item['success'] == 'timeout']\n",
    "    print 'Timeouts: '+ str(len(num_timeouts))\n",
    "    return causal_history\n",
    "\n",
    "def make_jepsen_log(a_causal_history, repo_path):\n",
    "    new_history = []\n",
    "    for item in a_causal_history:\n",
    "        \n",
    "        if item['type'] != 'sleep':\n",
    "        \n",
    "            #parse invoke op\n",
    "            invoke_output = 'INFO  jepsen.util - '\n",
    "            if item['host'] != 'nemisis':\n",
    "                invoke_output = invoke_output + re.findall(r'\\d+', item['host'])[0] + '     '\n",
    "                invoke_output = invoke_output + ':invoke '\n",
    "                invoke_output = invoke_output + ':'+ item['type']\n",
    "                if item['type'] == 'read':\n",
    "                    invoke_output = invoke_output + '    nil'\n",
    "                if item['type'] == 'write':\n",
    "                    invoke_output = invoke_output + '    ' + item['input'] + ''\n",
    "                if item['type'] == 'cas':\n",
    "                    invoke_output = invoke_output + '    ' + item['input'] + ''          \n",
    "            \n",
    "            else:\n",
    "                if item['type'] == 'partition':\n",
    "                    invoke_output = invoke_output + ':nemesis    :info    :start    nil'\n",
    "                if item['type'] == 'join':\n",
    "                    invoke_output = invoke_output + ':nemesis    :info    :stop    nil'\n",
    "            new_history.append({'output':invoke_output, 'timestamp':item['invoke_timestamp']})\n",
    "                \n",
    "            #parse completion op\n",
    "            compl_output = 'INFO  jepsen.util - '  \n",
    "            if item['host'] != 'nemisis':\n",
    "                compl_output = compl_output + re.findall(r'\\d+', item['host'])[0] + '     '\n",
    "                \n",
    "                if item['success'] == 'timeout':\n",
    "                    compl_output = compl_output + ':info   '\n",
    "                    compl_output = compl_output + ':'+ item['type']\n",
    "                    compl_output = compl_output + '   :timed-out'\n",
    "                else: \n",
    "                    compl_output = compl_output + ':' + item['success'] + '   '\n",
    "                    compl_output = compl_output + ':'+ item['type']\n",
    "                    if item['type'] == 'cas':\n",
    "                        compl_output = compl_output + '   ' + item['input'] + ''\n",
    "                        \n",
    "                    if item['type'] == 'write':\n",
    "                        compl_output = compl_output +  '   ' + item['input']  + ''\n",
    "                        \n",
    "                    if item['type'] == 'read' and item['success'] == 'ok':\n",
    "                        compl_output = compl_output +  '   ' + item['output']  + ''\n",
    "                        \n",
    "                    if item['type'] == 'read' and item['success'] != 'ok':\n",
    "                        compl_output = compl_output + '   nil'\n",
    "                        \n",
    "            else:\n",
    "                if item['type'] == 'partition':\n",
    "                    compl_output = compl_output + ':nemesis    :info    :start    '+item['output']\n",
    "                if item['type'] == 'join':\n",
    "                    compl_output = compl_output + ':nemesis    :info    :stop    \"fully connected\"'\n",
    "                \n",
    "            \n",
    "            new_history.append({'output':compl_output, 'timestamp':item['compl_timestamp']})\n",
    "    text_file = open(repo_path+\"/linearizability-checker/logs/test_0.log\", \"w\")\n",
    "    for item in sorted(new_history, key=lambda k: k['timestamp']):\n",
    "        #print(item['output']+'\\n')\n",
    "        text_file.write(item['output']+'\\n')\n",
    "    text_file.close()\n",
    "    \n",
    "def run_linearizability_checker(repo_path):\n",
    "    proc = subprocess.Popen([\"(cd \"+repo_path+\"/linearizability-checker && make lin-checker)\"], shell=True,\n",
    "             stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n",
    "    try:\n",
    "        a = str(proc.communicate()[1])\n",
    "        value = re.search('fail', a)\n",
    "        if value.group(0) == 'fail':\n",
    "            print 'Fail: history is not linearizable :('\n",
    "            return 0\n",
    "        else:\n",
    "            print 'Pass: history is linearizable :)'\n",
    "            return 1\n",
    "    \n",
    "    except:\n",
    "        print 'Pass: history is linearizable :)'\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Setup Test #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create Docker client\n",
    "# Be sure to update the client_cert paths and base_url value\n",
    "cert_path = 'path/to/cert'\n",
    "key_path = 'path/to/key'\n",
    "base_url = 'https://XXX.XXX.XXX.XXX:YYYY'\n",
    "\n",
    "tls = docker.tls.TLSConfig(client_cert=(cert_path, key_path), verify=False)\n",
    "docker_client = docker.Client(base_url=base_url, tls=tls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set repo path\n",
    "# Be sure to update the path\n",
    "repo_path = 'path/to/hard-tester'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensure there are no running execution clusters from previous runs\n",
    "stop_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Destroy Docker containers from previous runs\n",
    "stop_blockade(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start Docker containers\n",
    "start_blockade(5, repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install extra dependencies in the Docker containers\n",
    "setup_nodes(docker_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start execution cluster controller\n",
    "start_cluster_controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start execution cluster engines in each Docker container\n",
    "start_cluster_engines(docker_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cluster Riak and return the cluster nodes, or just get the Riak nodes\n",
    "nodes = riak_cluster(docker_client)\n",
    "#nodes = get_riak_nodes(docker_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start parallel executors and import Riak in each executor engine\n",
    "para_exec = start_parallel_executor()\n",
    "with para_exec.sync_imports():\n",
    "    import riak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up test\n",
    "bucket_name = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(10))\n",
    "key_name = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(5))\n",
    "value_name = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(5))\n",
    "\n",
    "# Start tests\n",
    "raw_hist = start_consitency_test(para_exec, nodes, 100, bucket_name, key_name, value_name, 1000, repo_path)\n",
    "\n",
    "# Parse raw history to create a causal history, order by invocation and completion timestamp\n",
    "causal_hist = make_causal_history(raw_hist)\n",
    "\n",
    "# Create Jepsen log from causal history\n",
    "make_jepsen_log(causal_hist, repo_path)\n",
    "\n",
    "# Run linearizability checker on the faux Jepsen log\n",
    "res = run_linearizability_checker(repo_path)\n",
    "results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
